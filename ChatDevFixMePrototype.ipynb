{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krognus/ChatWebDev/blob/master/ChatDevFixMePrototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rEmejz3UpKO",
        "outputId": "199c7e7c-24aa-4331-e9f8-b0b25da11b6c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, orjson, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 orjson-3.10.16 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NszmZSeizEdf"
      },
      "source": [
        "Snippet for project manager:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhux5jqIor2k"
      },
      "source": [
        "The following window is the snippet provided to the Ai Society upon initiating a porject:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cwep7w5LzWl9"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "snippet = \"\"\"\n",
        "/**\n",
        " * Game of Life\n",
        " * by Joan Soler-Adillon.\n",
        " *\n",
        " * Press SPACE BAR to pause and change the cell's values\n",
        " * with the mouse. On pause, click to activate/deactivate\n",
        " * cells. Press 'R' to randomly reset the cells' grid.\n",
        " * Press 'C' to clear the cells' grid. The original Game\n",
        " * of Life was created by John Conway in 1970.\n",
        " *\n",
        " * Programmed in Java using the Processing 4 IDE\n",
        " */\n",
        "\n",
        "// Size of cells\n",
        "int cellSize = 7;\n",
        "\n",
        "// How likely for a cell to be alive at start (in percentage)\n",
        "float probabilityOfAliveAtStart = 15;\n",
        "\n",
        "// Variables for timer\n",
        "int interval = 33;\n",
        "int lastRecordedTime = 0;\n",
        "\n",
        "// Colors for active/inactive cells\n",
        "color alive = color(0, 200, 0);\n",
        "color dead = color(0);\n",
        "\n",
        "// Array of cells\n",
        "int[][] cells;\n",
        "// Buffer to record the state of the cells and use this\n",
        "// while changing the others in the interations\n",
        "int[][] cellsBuffer;\n",
        "\n",
        "// Pause\n",
        "boolean pause = false;\n",
        "\n",
        "void setup() {\n",
        "  size (512, 512);\n",
        "\n",
        "  // Instantiate arrays\n",
        "  cells = new int[width/cellSize][height/cellSize];\n",
        "  cellsBuffer = new int[width/cellSize][height/cellSize];\n",
        "\n",
        "  // This stroke will draw the background grid\n",
        "  stroke(48);\n",
        "\n",
        "  noSmooth();\n",
        "\n",
        "  // Initialization of cells\n",
        "  for (int x=0; x<width/cellSize; x++) {\n",
        "    for (int y=0; y<height/cellSize; y++) {\n",
        "      float state = random (100);\n",
        "      if (state > probabilityOfAliveAtStart) {\n",
        "        state = 0;\n",
        "      }\n",
        "      else {\n",
        "        state = 1;\n",
        "      }\n",
        "      cells[x][y] = int(state); // Save state of each cell\n",
        "    }\n",
        "  }\n",
        "  // Fill in black in case cells don't cover all the windows\n",
        "  background(0);\n",
        "}\n",
        "\n",
        "\n",
        "void draw() {\n",
        "\n",
        "  //Draw grid\n",
        "  for (int x=0; x<width/cellSize; x++) {\n",
        "    for (int y=0; y<height/cellSize; y++) {\n",
        "      if (cells[x][y]==1) {\n",
        "        fill(alive); // If alive\n",
        "      }\n",
        "      else {\n",
        "        fill(dead); // If dead\n",
        "      }\n",
        "      rect (x*cellSize, y*cellSize, cellSize, cellSize);\n",
        "    }\n",
        "  }\n",
        "  // Iterate if timer ticks\n",
        "  if (millis()-lastRecordedTime>interval) {\n",
        "    if (!pause) {\n",
        "      iteration();\n",
        "      lastRecordedTime = millis();\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Create  new cells manually on pause\n",
        "  if (pause && mousePressed) {\n",
        "    // Map and avoid out of bound errors\n",
        "    int xCellOver = int(map(mouseX, 0, width, 0, width/cellSize));\n",
        "    xCellOver = constrain(xCellOver, 0, width/cellSize-1);\n",
        "    int yCellOver = int(map(mouseY, 0, height, 0, height/cellSize));\n",
        "    yCellOver = constrain(yCellOver, 0, height/cellSize-1);\n",
        "\n",
        "    // Check against cells in buffer\n",
        "    if (cellsBuffer[xCellOver][yCellOver]==1) { // Cell is alive\n",
        "      cells[xCellOver][yCellOver]=0; // Kill\n",
        "      fill(dead); // Fill with kill color\n",
        "    }\n",
        "    else { // Cell is dead\n",
        "      cells[xCellOver][yCellOver]=1; // Make alive\n",
        "      fill(alive); // Fill alive color\n",
        "    }\n",
        "  }\n",
        "  else if (pause && !mousePressed) { // And then save to buffer once mouse goes up\n",
        "    // Save cells to buffer (so we opeate with one array keeping the other intact)\n",
        "    for (int x=0; x<width/cellSize; x++) {\n",
        "      for (int y=0; y<height/cellSize; y++) {\n",
        "        cellsBuffer[x][y] = cells[x][y];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void iteration() { // When the clock ticks\n",
        "  // Save cells to buffer (so we opeate with one array keeping the other intact)\n",
        "  for (int x=0; x<width/cellSize; x++) {\n",
        "    for (int y=0; y<height/cellSize; y++) {\n",
        "      cellsBuffer[x][y] = cells[x][y];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Visit each cell:\n",
        "  for (int x=0; x<width/cellSize; x++) {\n",
        "    for (int y=0; y<height/cellSize; y++) {\n",
        "      // And visit all the neighbours of each cell\n",
        "      int neighbours = 0; // We'll count the neighbours\n",
        "      for (int xx=x-1; xx<=x+1;xx++) {\n",
        "        for (int yy=y-1; yy<=y+1;yy++) {\n",
        "          if (((xx>=0)&&(xx<width/cellSize))&&((yy>=0)&&(yy<height/cellSize))) { // Make sure you are not out of bounds\n",
        "            if (!((xx==x)&&(yy==y))) { // Make sure to to check against self\n",
        "              if (cellsBuffer[xx][yy]==1){\n",
        "                neighbours ++; // Check alive neighbours and count them\n",
        "              }\n",
        "            } // End of if\n",
        "          } // End of if\n",
        "        } // End of yy loop\n",
        "      } //End of xx loop\n",
        "      // We've checked the neigbours: apply rules!\n",
        "      if (cellsBuffer[x][y]==1) { // The cell is alive: kill it if necessary\n",
        "        if (neighbours < 1 || neighbours > 5) {\n",
        "          cells[x][y] = 0; // Die unless it has 2 or 4 neighbours\n",
        "        }\n",
        "      }\n",
        "      else { // The cell is dead: make it live if necessary\n",
        "        if (neighbours == 3 ) {\n",
        "          cells[x][y] = 1; // Only if it has 3 neighbours\n",
        "        }\n",
        "      } // End of if\n",
        "    } // End of y loop\n",
        "  } // End of x loop\n",
        "} // End of function\n",
        "\n",
        "void keyPressed() {\n",
        "  if (key=='r' || key == 'R') {\n",
        "    // Restart: reinitialization of cells\n",
        "    for (int x=0; x<width/cellSize; x++) {\n",
        "      for (int y=0; y<height/cellSize; y++) {\n",
        "        float state = random (100);\n",
        "        if (state > probabilityOfAliveAtStart) {\n",
        "          state = 0;\n",
        "        }\n",
        "        else {\n",
        "          state = 1;\n",
        "        }\n",
        "        cells[x][y] = int(state); // Save state of each cell\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  if (key==' ') { // On/off of pause\n",
        "    pause = !pause;\n",
        "  }\n",
        "  if (key=='c' || key == 'C') { // Clear all\n",
        "    for (int x=0; x<width/cellSize; x++) {\n",
        "      for (int y=0; y<height/cellSize; y++) {\n",
        "        cells[x][y] = 0; // Save all to zero\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "Project Status: [incomplete]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ia2Pon-6jov"
      },
      "source": [
        "ChatDev AI Society Prototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "jE9AYALIRsvK",
        "outputId": "1f0363d9-d671-4635-83c8-a0e7c774412e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: https://huggingface-projects-llama-2-7b-chat.hf.space ✔\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Parameter `temperature` is not a valid key-word argument. Please click on 'view API' in the footer of the Gradio app to see usage.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-241711cc5eac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mproject_manager_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_manager_job\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"The Project is listed as {projectStatus}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mproject_manager_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_manager_job\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"make sure to write the [incomplete] tag, so the coder understands he's been assigned a task.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mproject_manager_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllamaChat4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_manager_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_manager_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0minitOnCode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mcoder_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoder_prompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Never modify, delete, nor omit the {projectStatus} tag.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-241711cc5eac>\u001b[0m in \u001b[0;36mcreate_worker\u001b[0;34m(api_name, initial_prompt, task_prompt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   result = client.predict(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"**Job:**{initial_prompt}\\n**Task:**{task_prompt}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Combine prompts correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# request=0.95, # Temperature parameter - This parameter is likely incorrect for this API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, api_name, fn_index, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_fn_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         return self.submit(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         ).result()\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/client.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, api_name, fn_index, result_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/utils.py\u001b[0m in \u001b[0;36mconstruct_args\u001b[0;34m(parameters_info, args, kwargs)\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwarg_arg_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m   1274\u001b[0m                 \u001b[0;34mf\"Parameter `{key}` is not a valid key-word argument. Please click on 'view API' in the footer of the Gradio app to see usage.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Parameter `temperature` is not a valid key-word argument. Please click on 'view API' in the footer of the Gradio app to see usage."
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from gradio_client import Client\n",
        "\n",
        "# Function to create AI society worker instances\n",
        "# (ysharma)Parameter 3 is the token length. 512 is short. 4096 is max?\n",
        "# (huggingface-projects)Has 7 parameters.\n",
        "###Curently commenting out different workers until I can figure out how to automatically know which apiset to use.\n",
        "\n",
        "###Using ysharma(llama2) - This one works, but uses up gpu fast.\n",
        "###def create_worker(api_name, initial_prompt, task_prompt):\n",
        "###    client = Client(api_name)\n",
        "###    result = client.predict(\n",
        "###        message=f\"{initial_prompt}\\n**Task:**\\n{task_prompt}\",\n",
        "###        request=0.95,\n",
        "###        param_3=512,\n",
        "###        api_name=\"/chat\"\n",
        "###    )\n",
        "###    return result\n",
        "\n",
        "#Using huggingface-projects(llama4) - Trying new space...\n",
        "\n",
        "\n",
        "def create_worker(api_name, initial_prompt, task_prompt):\n",
        "  client = Client(api_name)\n",
        "  result = client.predict(\n",
        "        message=f\"**Job:**{initial_prompt}\\n**Task:**{task_prompt}\", # Combine prompts correctly\n",
        "        # request=0.95, # Temperature parameter - This parameter is likely incorrect for this API\n",
        "        temperature=0.95, # Use temperature instead of request for temperature control\n",
        "        max_new_tokens=512, # Use max_new_tokens for token length instead of param_3\n",
        "        api_name=\"/chat\" # Specify API endpoint\n",
        "  )\n",
        "  return result\n",
        "\n",
        "language = \"python\"\n",
        "IDE = \".ipynb\"\n",
        "template = snippet\n",
        "param_3 = \"1024\" #tokenMAx\n",
        "###################################################\n",
        "### INIT TASK PROMPTS TO PROJECT MANAGER\n",
        "###################################################\n",
        "#project_manager_job0=f\"read this script: {snippet}. Code in javascript. Implement a feature to tile the 80x80 canvas in an 8x8 grid where each grid position contains 1 instance of the game. In total there should be 64 canvasses arrayed in a grid, each canvas running the code snippet provided earlier.\"\n",
        "#project_manager_job1=f\"read this script: {snippet}. It was Coded in java, on the {IDE} IDE. It is a game of life simulator using a unique ruleset to make tiling patterns. I need new features added to the script. 1a. A tileset saving function. 1b. A way to draw and save templates of drawings made with the MouseIsDown events. 2. A way to pause, reverse, and forward step events. 3. A way to export tilesets as gcode, to be interpreted by a slicing proram(Cura).\"\n",
        "#Implement a feature to tile the 80x80 canvas in an 8x8 grid where each grid position contains 1 instance of the game.\n",
        "#In total there should be 64 canvasses arrayed in a grid, each canvas running the code snippet provided earlier.\n",
        "project_manager_job=f\"You've been **TASKED** a project. Here's the outline script: {snippet}. It was Coded in {language}, on the {IDE} IDE.\"\n",
        "#Somewhere here we need to get the last pastebin results and loop it back to project manager job in the new snippet.\n",
        "#f=\"In this array {pastebin_list}, grab the most recent paste url, which will be the last item in the array. This is the previous tasks' results.\n",
        "#f=\"Read this last job's report: {final_result}.\"\"\n",
        "#project_manager_job=\n",
        "###################################################\n",
        "### COMMENTS\n",
        "###################################################\n",
        "#abacab\n",
        "# Define prompts for each role\n",
        "#output has this format\\n [1 sentence summary of assigned task] [results of task] [next steps for further improvement] [new task assignment]\n",
        "#interior inference  variables for taskers to follow, and code to interpret in [brackets]\n",
        "#Project Manager's internal variables\n",
        "###workers=[manager, tasker, coder, interpreter, reviewer, summarizer, delegater]\n",
        "#[inputProject] user prompt's contents\n",
        "#[project] commandified procedure of [inputProject]\n",
        "#[delegate]\n",
        "\n",
        "#Tasker's internal variables\n",
        "#[task] Job Assignment\n",
        "#[assummary] Assignment Summary\n",
        "#[results] Final results of assigned task\n",
        "#[nexttask] Next steps for further improvement\n",
        "#[delegate] workers to pass [nexttask] to\n",
        "#[memosend] Next worker's assignment instructions'\n",
        "\n",
        "#############################################################\n",
        "### PEPTALKS\n",
        "#############################################################\n",
        "#Interpreter's internal variables, their job role(coder, project manager, reviewer), and their role's duties.\n",
        "\n",
        "coder_prompt = f\"\"\"\n",
        "You are the Coder AI. Your job is to write all code for the project.\n",
        "All output must be in {language}. You've recieved a new assignment from the project managers a program outline from the project manager.\n",
        " the program outline contains @placeholders for the program's functions.\n",
        " Afterward, focus on the @placeholders and write the code described for each of\n",
        " the @placeholders as concisely and completly as possible. Finally, remove the @placeholder\n",
        " ONLY for the codeblock you've completed. Continue for the rest of the @placeholders yet to be coded,\n",
        " unitil you've written code for ALL of the @placeholder tasks. send the results to the reviewer.\n",
        " Your output should be no less than {param_3} tokens in length.\n",
        " \"\"\"\n",
        "\n",
        "reviewer_prompt = f\"\"\"\n",
        "You are the Code Reviewer AI.\n",
        "Your job is to scan and fix the code for errors.\n",
        "Afterward, print the entire contents of script with all implementations.\n",
        "Finally,  if there are any @placeholders remaining, declare the program\n",
        "is not yet complete.  Only if there are no @placeholders remaining,\n",
        " state the program is fully complete.\n",
        " Your output should be no less than {param_3} tokens in length.\"\n",
        "\"\"\"\n",
        "\n",
        "project_manager_prompt = f\"\"\"\n",
        "You are the {language} Admin AI. Your job is to create an outline script containing commented concise instruction prompts\n",
        "for the needed code. All output should be in concise, thorough code: if a comment is necessary make it brief.\n",
        ". as you are fulfilling your **TASKS**, create new features to be implemented and\n",
        "write as much code as you can to flesh out the features aspects. do not write things like \"XX code goes here\"\" or heavy comments:\n",
        "simply write a brief sentence with instructions for the coder and tag the comment with **TASK**. Remember, completeness is key:\n",
        "any multitude of comments or yet unimplemented code will result in terminations.(avoid at all costs!) You will then write one @placeholder tag for\n",
        "the next major feature to implement. Stay within scope but don't output low quality code. send results to coder.\n",
        "\"\"\"\n",
        "\n",
        "boolean_dude_prompt = f\"\"\"\n",
        "you are a boolean AI. You check the reviewer's results. Evaluate: is the project finished according to reviewer?\n",
        "Does the reviewer declare the program is [incomplete]?\n",
        "if reviewer does not mention the status of the program immediately output 0 and send results to coder. Likewise,\n",
        "if you see any incomplete code immediately output 0.\n",
        "your job output is a single boolean byte. '0' or '1'. You'll be coming to the conclusion '0' often,\n",
        "you must be very critical of the scripts completedness as you are the finalizer.\n",
        "\"\"\"\n",
        "\n",
        "#prompts_array = [project_manager_prompt, prompt_fixer_prompt]\n",
        "\n",
        "#############################################################\n",
        "### LinkDen\n",
        "#############################################################\n",
        "#Spaces API clients\n",
        "llamaChat2=\"ysharma/Chat_with_Meta_llama3_8b\"\n",
        "llamaChat4=\"huggingface-projects/llama-2-7b-chat\"\n",
        "llamaChat3=\"huggingface-projects/llama-2-13b-chat\"\n",
        "llamaChat=\"eswardivi/llama3-8b-dhenu-0.1\"\n",
        "\n",
        "#############################################################\n",
        "### INIT VARIABLES\n",
        "#############################################################\n",
        "tasker_job = \"\"\n",
        "input_output_job = \"\"\n",
        "#prompt_fixer_job = tasker_prompt\n",
        "initpost = \"true\"\n",
        "initOnCode = \"true\"\n",
        "projectStatus = \"[incomplete]\"\n",
        "#if not pastebin_list:\n",
        "#  pastebin_list = []\n",
        "\n",
        "\n",
        "############################################################\n",
        "### ATTACH BRAINS TO WORKERS\n",
        "############################################################\n",
        "# Create instances of different AI workers\n",
        "#tasker_result = create_worker(llamaChat, tasker_prompt, tasker_job)\n",
        "while projectStatus == \"[incomplete]\":\n",
        "    if initpost == \"true\":\n",
        "        initpost = \"false\"\n",
        "        project_manager_job = project_manager_job + f\"The Project is listed as {projectStatus}.\"\n",
        "        project_manager_job = project_manager_job + f\"make sure to write the [incomplete] tag, so the coder understands he's been assigned a task.\"\n",
        "        project_manager_result = create_worker(llamaChat4, project_manager_prompt, project_manager_job)\n",
        "        initOnCode == \"true\"\n",
        "    coder_prompt = coder_prompt + f\"Never modify, delete, nor omit the {projectStatus} tag.\"\n",
        "    coder_result = create_worker(llamaChat4, coder_prompt, project_manager_result)\n",
        "    initOnCode = \"false\"\n",
        "    coder_prompt + f\"\"\"The code reviewer has declared your tasks are incomplete.\n",
        "\n",
        "        Finish developing the code; do not skip commented outlines, your job is always to flesh out the outlines and write all the major components.\n",
        "        fill in all conmented sections with concise, thorough code that is true to the original project scope.\n",
        "        use best judgement: if you see any incomplete code, your job is not done.\n",
        "        if you see commented sections, @placeholders, or plaintext outlines your job is incomplete.\n",
        "        \"\"\"\n",
        "else:\n",
        "  projectStatus = \"[incomplete]\"\n",
        "initpost = \"true\"\n",
        "\n",
        "while projectStatus == \"[incomplete]\":\n",
        "    if initpost == \"true\":\n",
        "        initpost = \"false\"\n",
        "        project_manager_job = \"The Project is listed as [incomplete].\"\n",
        "        project_manager_job += \" Make sure to write the project description, the steps needed to complete the project, and the expected completion date.\"\n",
        "        project_manager_prompt = create_worker(project_manager_job)\n",
        "\n",
        "    project_manager_output = llamaChat4(project_manager_prompt)\n",
        "    project_description = project_manager_output[\"project_description\"]\n",
        "    project_steps = project_manager_output[\"project_steps\"]\n",
        "    expected_completion_date = project_manager_output[\"expected_completion_date\"]\n",
        "\n",
        "    coder_job = \"Here is the project description: \" + project_description\n",
        "    coder_job += \" Here are the steps needed to complete the project: \" + project_steps\n",
        "    coder_job += \" The expected completion date is: \" + expected_completion_date\n",
        "    coder_job += \" Please write the Python code to complete the project.\"\n",
        "    coder_prompt = create_worker(coder_job)\n",
        "\n",
        "    coder_output = llamaChat4(coder_prompt)\n",
        "    python_code = coder_output[\"python_code\"]\n",
        "\n",
        "    reviewer_job = \"Here is the project description: \" + project_description\n",
        "    reviewer_job += \" Here are the steps needed to complete the project: \" + project_steps\n",
        "    reviewer_job += \" The expected completion date is: \" + expected_completion_date\n",
        "    reviewer_job += \" Here is the Python code to complete the project: \" + python_code\n",
        "    reviewer_job += \" Please review the code and make sure it meets the requirements of the project.\"\n",
        "    reviewer_prompt = create_worker(reviewer_job)\n",
        "\n",
        "    reviewer_output = llamaChat4(reviewer_prompt)\n",
        "    review_comments = reviewer_output[\"review_comments\"]\n",
        "\n",
        "    # Check if the project is complete\n",
        "    if review_comments == \"The project is complete.\":\n",
        "        projectStatus = \"[complete]\"\n",
        "        print(\"The project is complete.\")\n",
        "    else:\n",
        "        # Update the project manager with the review comments\n",
        "        project_manager_job = \"Here are the review comments:\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "projectStatus == \"[incomplete]\"\n",
        "#boolean_result = create_worker(llamaChat4, boolean_dude_prompt, reviewer_result)\n",
        "#input_output_result = create_worker(llamaChat, input_output_prompt, task_prompt)\n",
        "#prompt_fixer_result = create_worker(llamaChat, prompt_fixer_prompt, prompt_fixer_job)\n",
        "\n",
        "reviewer_job=coder_result\n",
        "boolean_job = \"\"\n",
        "\n",
        "#######################################################################\n",
        "### SUMMARIZER AI\n",
        "#######################################################################\n",
        "# Function to collate all called tasks' results, title the output's worker.\n",
        "# Prettyprintifying, please wait...\n",
        "def summarizer(project_manager_result, coder_result, reviewer_result):\n",
        "    # Summarize results and infer future tasks\n",
        "    summarized_result = f\"\"\"\n",
        "    **Manager AI:** {project_manager_result}\n",
        "    **Coder AI:** {coder_result}\n",
        "    **Reviewer AI:** {reviewer_result}\n",
        "    \"\"\"\n",
        "    return summarized_result\n",
        "\n",
        "# Call the summarizer function\n",
        "final_result = summarizer(project_manager_result, coder_result, reviewer_result)\n",
        "\n",
        "#summarizer_prompt = final_result+\"Summarize this workflow in a single paragraph.\"\n",
        "#summarizer_result = create_worker(llamaChat, summarizer_prompt, task_prompt)\n",
        "\n",
        "# Print or further process the final result\n",
        "#print(final_result)\n",
        "\n",
        "#############################################################\n",
        "### BOOLEAN DUDE AI\n",
        "#############################################################\n",
        "# Function to check Boolean Dude's response for 0 or 1 contained in text, returns True or False\n",
        "# This complexication of inference is for further project workflow\n",
        "# FLOWCHARTS WHEN????\n",
        "\n",
        "print(f\"Based on the inputs received, boolean dude has said {boolean_result}.\")\n",
        "def boolean_dude_response(boolean_result):\n",
        "  #Check if Boolean Dude has said \"0\", return False\n",
        "  if \"0\" in boolean_result:\n",
        "    return False\n",
        "  # Check if the string contains \"1\" (only if \"0\" wasn't found earlier)\n",
        "  elif \"1\" in boolean_result:\n",
        "    return True\n",
        "is_project_done = bool(boolean_result)\n",
        "if is_project_done == False:\n",
        "  project_manager_job.append(f\"Here's a previous task that needs more work: {final_result}\")\n",
        "\n",
        "####################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Function to create a worker\n",
        "def create_worker(api_name, initial_prompt, task_prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=api_name,\n",
        "        prompt=f\"**Job:**\\n{initial_prompt}\\n**Task:**\\n{task_prompt}\",\n",
        "        max_tokens=1024,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        frequency_penalty=0.5,\n",
        "        presence_penalty=1.2\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Function to check project status (placeholder for actual implementation)\n",
        "def check_project_status(result):\n",
        "    # Placeholder logic to simulate project status update.\n",
        "    # Replace with actual logic to determine if the project is complete or incomplete.\n",
        "    if \"complete\" in result.lower():\n",
        "        return \"[complete]\"\n",
        "    else:\n",
        "        return \"[incomplete]\"\n",
        "\n",
        "# Function for project manager task\n",
        "def project_manager_task(api_name, project_manager_prompt, project_status):\n",
        "    project_manager_job = \"The Project is listed as [incomplete].\"\n",
        "    project_manager_job += \" Make sure to write the [incomplete] tag, so the coder understands he's been assigned a task.\"\n",
        "    return create_worker(api_name, project_manager_prompt, project_manager_job)\n",
        "\n",
        "# Function for coder task\n",
        "def coder_task(api_name, coder_prompt, project_manager_result, project_status):\n",
        "    coder_task_prompt = coder_prompt + f\" Never modify, delete, nor omit the {project_status} tag.\"\n",
        "    coder_result = create_worker(api_name, coder_task_prompt, project_manager_result)\n",
        "    coder_task_prompt += \"\"\"\n",
        "    The code reviewer has declared your tasks are incomplete.\n",
        "\n",
        "    Finish developing the code; do not skip commented outlines, your job is always to flesh out the outlines and write all the major components.\n",
        "    Fill in all commented sections with concise, thorough code that is true to the original project scope.\n",
        "    Use best judgement: if you see any incomplete code, your job is not done.\n",
        "    If you see commented sections, @placeholders, or plaintext outlines your job is incomplete.\n",
        "    \"\"\"\n",
        "    return create_worker(api_name, coder_task_prompt, coder_result)\n",
        "\n",
        "# Function for reviewer task\n",
        "def reviewer_task(api_name, reviewer_prompt, coder_result):\n",
        "    return create_worker(api_name, reviewer_prompt, coder_result)\n",
        "\n",
        "# Main function to iterate the project development process\n",
        "def iterate_project_development(api_name, project_manager_prompt, coder_prompt, reviewer_prompt):\n",
        "    project_status = \"[incomplete]\"\n",
        "    initpost = \"true\"\n",
        "\n",
        "    while project_status == \"[incomplete]\":\n",
        "        if initpost == \"true\":\n",
        "            initpost = \"false\"\n",
        "            project_manager_result = project_manager_task(api_name, project_manager_prompt, project_status)\n",
        "\n",
        "        coder_result = coder_task(api_name, coder_prompt, project_manager_result, project_status)\n",
        "        reviewer_result = reviewer_task(api_name, reviewer_prompt, coder_result)\n",
        "        coder_result = coder_task(api_name, coder_prompt, reviewer_result, project_status)\n",
        "\n",
        "        # Check the project status based on the latest coder result\n",
        "        project_status = check_project_status(coder_result)\n",
        "\n",
        "    return coder_result\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have set your OpenAI API key in the environment or using `openai.api_key = 'your-api-key'`\n",
        "api_name = \"text-davinci-003\"\n",
        "project_manager_prompt = \"Project Manager's Initial Prompt\"\n",
        "coder_prompt = \"Coder's Initial Prompt\"\n",
        "reviewer_prompt = \"Reviewer's Initial Prompt\"\n",
        "\n",
        "final_result = iterate_project_development(api_name, project_manager_prompt, coder_prompt, reviewer_prompt)\n",
        "print(f\"Final Result:\\n{final_result}\")\n"
      ],
      "metadata": {
        "id": "0rCsj_NKKcD7",
        "outputId": "9cbdecfc-a441-4223-cb06-021efe52b98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-47b05cae0961>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Function to create a worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     response = openai.Completion.create(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEibF2hLGqnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from gradio_client import Client\n",
        "\n",
        "# Function to create AI society worker instances\n",
        "def create_worker(api_name, initial_prompt, task_prompt):\n",
        "    client = Client(api_name)\n",
        "    result = client.predict(\n",
        "        message=f\"**Job:**{initial_prompt}\",\n",
        "        request=f\"**Task:**{task_prompt}\",\n",
        "        param_3=1024,\n",
        "        param_4=0.7,\n",
        "        param_5=0.95,\n",
        "        param_6=50,\n",
        "        param_7=1.2,\n",
        "        api_name=\"/chat\"\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Define prompts and URLs\n",
        "language = \"python\"\n",
        "IDE = \".ipynb\"\n",
        "param_3 = \"1024\" #tokenMAx\n",
        "\n",
        "coder_prompt = f\"\"\"\n",
        "You are the Coder AI. Your job is to write all code for the project.\n",
        "All output must be in {language}. You've received a new assignment from the project managers with a program outline.\n",
        "The program outline contains @placeholders for the program's functions.\n",
        "Afterward, focus on the @placeholders and write the code described for each of\n",
        "the @placeholders as concisely and completely as possible. Finally, remove the @placeholder\n",
        "ONLY for the codeblock you've completed. Continue for the rest of the @placeholders yet to be coded,\n",
        "until you've written code for ALL of the @placeholder tasks. Send the results to the reviewer.\n",
        "Your output should be no less than {param_3} tokens in length.\n",
        "\"\"\"\n",
        "\n",
        "reviewer_prompt = f\"\"\"\n",
        "You are the Code Reviewer AI.\n",
        "Your job is to scan and fix the code for errors.\n",
        "Afterward, print the entire contents of the script with all implementations.\n",
        "Finally, if there are any @placeholders remaining, declare the program\n",
        "is not yet complete. Only if there are no @placeholders remaining,\n",
        "state the program is fully complete.\n",
        "Your output should be no less than {param_3} tokens in length.\n",
        "\"\"\"\n",
        "\n",
        "project_manager_prompt = f\"\"\"\n",
        "You are the {language} Admin AI. Your job is to create an outline script containing concise instruction prompts\n",
        "for the needed code. All output should be in concise, thorough code: if a comment is necessary, make it brief.\n",
        "As you are fulfilling your **TASKS**, create new features to be implemented and\n",
        "write as much code as you can to flesh out the features aspects. Do not write things like \"XX code goes here\" or heavy comments:\n",
        "simply write a brief sentence with instructions for the coder and tag the comment with **TASK**. Remember, completeness is key:\n",
        "any multitude of comments or yet unimplemented code will result in terminations (avoid at all costs!). You will then write one @placeholder tag for\n",
        "the next major feature to implement. Stay within scope but don't output low-quality code. Send results to the coder.\n",
        "\"\"\"\n",
        "\n",
        "boolean_dude_prompt = f\"\"\"\n",
        "You are a Boolean AI. You check the reviewer's results. Evaluate: is the project finished according to the reviewer?\n",
        "Does the reviewer declare the program is [incomplete]?\n",
        "If the reviewer does not mention the status of the program, immediately output 0 and send results to the coder. Likewise,\n",
        "if you see any incomplete code, immediately output 0.\n",
        "Your job output is a single boolean byte: '0' or '1'. You'll be coming to the conclusion '0' often,\n",
        "you must be very critical of the script's completeness as you are the finalizer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize variables\n",
        "projectStatus = \"[incomplete]\"\n",
        "\n",
        "def worker_task(api_name, prompt, job):\n",
        "    return create_worker(api_name, prompt, job)\n",
        "\n",
        "# Run parallel processing for initial project manager and coder jobs\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    while projectStatus == \"[incomplete]\":\n",
        "        if initpost == \"true\":\n",
        "            initpost = \"false\"\n",
        "            project_manager_job += f\"The Project is listed as {projectStatus}.\"\n",
        "            project_manager_job += f\"Make sure to write the [incomplete] tag, so the coder understands he's been assigned a task.\"\n",
        "\n",
        "            future_project_manager = executor.submit(worker_task, llamaChat4, project_manager_prompt, project_manager_job)\n",
        "            project_manager_result = future_project_manager.result()\n",
        "            initOnCode = \"true\"\n",
        "        coder_prompt += f\"Never modify, delete, nor omit the {projectStatus} tag.\"\n",
        "        future_coder = executor.submit(worker_task, llamaChat4, coder_prompt, project_manager_result)\n",
        "        coder_result = future_coder.result()\n",
        "        initOnCode = \"false\"\n",
        "\n",
        "        # Updating prompts and tasks for the next iteration\n",
        "        coder_prompt += f\"\"\"\n",
        "        The code reviewer has declared your tasks are incomplete.\n",
        "        Finish developing the code; do not skip commented outlines, your job is always to flesh out the outlines and write all major components.\n",
        "        Fill in all commented sections with concise, thorough code that is true to the original project scope.\n",
        "        Use best judgement: if you see any incomplete code, your job is not done.\n",
        "        If you see commented sections, @placeholders, or plaintext outlines your job is incomplete.\n",
        "        \"\"\"\n",
        "\n",
        "        reviewer_result = create_worker(llamaChat4, reviewer_prompt, coder_result)\n",
        "        coder_result = create_worker(llamaChat4, coder_prompt, reviewer_result)\n",
        "\n",
        "        # Boolean evaluation to determine if the project is done\n",
        "        boolean_result = create_worker(llamaChat4, boolean_dude_prompt, reviewer_result)\n",
        "        is_project_done = boolean_result == \"1\"\n",
        "\n",
        "        if is_project_done:\n",
        "            projectStatus = \"[complete]\"\n",
        "        else:\n",
        "            projectStatus = \"[incomplete]\"\n",
        "\n",
        "# Summarizer Function\n",
        "def summarizer(project_manager_result, coder_result, reviewer_result):\n",
        "    summarized_result = f\"\"\"\n",
        "    **Manager AI:** {project_manager_result}\n",
        "    **Coder AI:** {coder_result}\n",
        "    **Reviewer AI:** {reviewer_result}\n",
        "    \"\"\"\n",
        "    return summarized_result\n",
        "\n",
        "# Call the summarizer function\n",
        "final_result = summarizer(project_manager_result, coder_result, reviewer_result)\n",
        "\n",
        "# Print the final result\n",
        "print(final_result)\n"
      ],
      "metadata": {
        "id": "_IXr9uav4h4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ8FnwQppvip"
      },
      "source": [
        "Printout options from workers results:\n",
        "Pastebin\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2gE9wnlCphk"
      },
      "outputs": [],
      "source": [
        "###\"\"\"\n",
        "# can you rewrite this part to check to make sure pastebin_list exists or not. if its already an array with values, don't do anything. if it is an array that doesn't exist yet, create it(pastebin_list = [])\n",
        "# if pastebin_list is not empty\n",
        "#\n",
        "#else\n",
        "#pastebin_list = []\n",
        "###\"\"\"\"\n",
        "\n",
        "if not pastebin_list:\n",
        "  pastebin_list = []\n",
        "\n",
        "print(pastebin_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaeF3CYRq0vt"
      },
      "source": [
        "Scratchpad notes Procons script functioning:\n",
        "PROJECT MANAGER AI\n",
        "+worker projectmanager is correctly placing @placeholder functions for future filling out, and directing the coder AI to work on a specific task.\n",
        "\n",
        "CODER AI\n",
        "+worker coder is correctly following instruction to code the task assigned from @placeholder, rewrites the codeblock, and removes the @placeholder tag from the section its programmed.\n",
        "- after it outputs its codeblock script, it leaves the rest of the script unfinished, as it was tasked with completing only 1 @placeholder task.\n",
        "\n",
        "REVIEWER AI\n",
        "+outputs script as coded by Coder AI\n",
        "+lists tasks still needed to complete\n",
        "+evaluates if project is done or not\n",
        "-dosen't pass task on\n",
        "\n",
        "BOOLEAN DUDE AI\n",
        "\n",
        "\n",
        "NEXT STEPS\n",
        "divy out labors for @placeholder tasks, 1 worker per task-OR-unknown implementation- for each @placeholder task appointed by project manager, assign coder AI piecewise and re-merge final script\n",
        "\n",
        " *FLURRY OF KEYSTROKES* feat: Coder AI copies itself many times and performs a @placeholder code job per each copy, and remerges its mirror selves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaEHW2zHM5YM"
      },
      "outputs": [],
      "source": [
        "#print(project_manager_result)\n",
        "#print(coder_result)\n",
        "#print(reviewer_result)\n",
        "###\"\"\"\n",
        "###i want these 3 strings to be collated into a single string, with f, to also add a newline after each result string\n",
        "###\"\"\"\n",
        "\n",
        "summarized_result = f\"{project_manager_result}\\n{coder_result}\\n{reviewer_result}\"\n",
        "print(summarized_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUnpQSz-7N2k"
      },
      "source": [
        "Pastebin Results Post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "846HHLuq-mNA"
      },
      "outputs": [],
      "source": [
        "#Initate pastebin urls array - Only Run ONCE!\n",
        "#This array will hold the list of pastebins created\n",
        "pastebin_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBugGdfl_9gE"
      },
      "outputs": [],
      "source": [
        "#This script posts the contents of final_result to a new pastebin\n",
        "import requests\n",
        "\n",
        "# Replace with your Pastebin developer key\n",
        "developer_key = \"QNd8q3_4l6ThEW6mBvhAQh0sidkooSpb\"\n",
        "\n",
        "# String to be pasted\n",
        "text_to_paste = final_result  # Assuming final_result is plain text\n",
        "\n",
        "# Optional parameters\n",
        "paste_name = \"helloWorldResult\"  # Optional title for the paste\n",
        "paste_private = 0  # 0 (Public), 1 (Unlisted), 2 (Private - Requires login)\n",
        "\n",
        "# API endpoint URL\n",
        "url = \"https://pastebin.com/api/api_post.php\"\n",
        "\n",
        "# Data to send in the POST request\n",
        "data = {\n",
        "    \"api_option\": \"paste\",\n",
        "    \"api_dev_key\": developer_key,\n",
        "    \"api_paste_code\": text_to_paste,\n",
        "    \"api_paste_name\": paste_name,  # Include if desired\n",
        "    \"api_paste_private\": paste_private,\n",
        "}\n",
        "\n",
        "# Send POST request\n",
        "response = requests.post(url, data=data)\n",
        "\n",
        "# Check for successful response\n",
        "if response.status_code == 200:\n",
        "    # Paste created successfully\n",
        "    paste_url = response.text.strip()  # Assuming the response contains the URL directly\n",
        "\n",
        "    # Add URL to your list\n",
        "    pastebin_list.append(paste_url)\n",
        "\n",
        "    # Print success message\n",
        "    print(f\"Paste created successfully! URL: {paste_url}\")\n",
        "else:\n",
        "    print(f\"Error creating paste: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP1IWt5q7cLe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "write a python notebooks script to print the text from a string called \"hello_worldResults\"   with the pastebin api to create a new paste. add the url of the returned pastebin url for the new paste into an array called pastebinList. also, save the contents of the array into a .csv file.\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "# Replace with your Pastebin developer key\n",
        "developer_key = \"QNd8q3_4l6ThEW6mBvhAQh0sidkooSpb\"\n",
        "\n",
        "# String to be pasted\n",
        "text_to_paste = final_result\n",
        "\n",
        "# Optional parameters\n",
        "paste_name = \"helloWorldResult\"  # Optional title for the paste\n",
        "paste_private = 0  # 0 (Public), 1 (Unlisted), 2 (Private - Requires login)\n",
        "\n",
        "# API endpoint URL\n",
        "url = \"https://pastebin.com/api/api_post.php\"\n",
        "\n",
        "# Data to send in the POST request\n",
        "data = {\n",
        "    \"api_option\": \"paste\",\n",
        "    \"api_dev_key\": developer_key,\n",
        "    \"api_paste_code\": text_to_paste,\n",
        "    \"api_paste_name\": paste_name,  # Include if desired\n",
        "    \"api_paste_private\": paste_private,\n",
        "}\n",
        "\n",
        "# Send POST request\n",
        "response = requests.post(url, data=data)\n",
        "\n",
        "# Check for successful response\n",
        "if response.status_code == 200:\n",
        "    # Parse the response (assuming JSON format)\n",
        "    pastebin_data = response.json()\n",
        "    paste_url = pastebin_data[\"paste_url\"]\n",
        "\n",
        "    # Add URL to your list\n",
        "    pastebin_list.append(paste_url)\n",
        "\n",
        "    # Print success message\n",
        "    print(f\"Paste created successfully! URL: {paste_url}\")\n",
        "else:\n",
        "    print(f\"Error creating paste: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObvqH7A8ys3U"
      },
      "source": [
        "**Snippets**-------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_N8BwjL1Wxa"
      },
      "outputs": [],
      "source": [
        "snippet = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <title>Conway's Game of Life</title>\n",
        "  <style>\n",
        "    canvas {\n",
        "      border: 1px solid black;\n",
        "\n",
        "    }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <canvas id=\"gameCanvas\" width=\"80\" height=\"80\"></canvas>\n",
        "  <script>\n",
        "    // Set up the canvas\n",
        "    const canvas = document.getElementById(\"gameCanvas\");\n",
        "    const ctx = canvas.getContext(\"2d\");\n",
        "    const cellSize = 5;\n",
        "    const rows = canvas.height / cellSize;\n",
        "    const cols = canvas.width / cellSize;\n",
        "\n",
        "    // Create a 2D array to store the game state\n",
        "    let gameState = new Array(rows).fill(0).map(() => new Array(cols).fill(0));\n",
        "\n",
        "    // Helper function to generate a random color\n",
        "    function getRandomColor() {\n",
        "      const letters = \"0123456789ABCDEF\";\n",
        "      let color = \"#\";\n",
        "      for (let i = 0; i < 6; i++) {\n",
        "        color += letters[Math.floor(Math.random() * 16)];\n",
        "      }\n",
        "      return color;\n",
        "    }\n",
        "\n",
        "    // Helper function to initialize the game state with random cells\n",
        "    function initializeGameState() {\n",
        "      for (let i = 0; i < rows; i++) {\n",
        "        for (let j = 0; j < cols; j++) {\n",
        "          gameState[i][j] = Math.random() < 0.5 ? 0 : 1;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Helper function to draw the current state of the game\n",
        "    function drawGame() {\n",
        "      ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "      for (let i = 0; i < rows; i++) {\n",
        "        for (let j = 0; j < cols; j++) {\n",
        "          if (gameState[i][j] === 1) {\n",
        "            ctx.fillRect(j * cellSize, i * cellSize, cellSize, cellSize);\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Helper function to count the number of live neighbors for a cell\n",
        "    function countLiveNeighbors(x, y) {\n",
        "      let count = 0;\n",
        "      for (let i = -1; i <= 1; i++) {\n",
        "        for (let j = -1; j <= 1; j++) {\n",
        "          if (i === 0 && j === 0) continue;\n",
        "          const neighborX = x + i;\n",
        "          const neighborY = y + j;\n",
        "          if (neighborX >= 0 && neighborX < rows && neighborY >= 0 && neighborY < cols) {\n",
        "            count += gameState[neighborX][neighborY];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      return count;\n",
        "    }\n",
        "\n",
        "    // Function to update the game state based on the rules of the game\n",
        "    function updateGameState() {\n",
        "      const newGameState = new Array(rows).fill(0).map(() => new Array(cols).fill(0));\n",
        "      for (let i = 0; i < rows; i++) {\n",
        "        for (let j = 0; j < cols; j++) {\n",
        "          const liveNeighbors = countLiveNeighbors(i, j);\n",
        "          if (gameState[i][j] === 1) {\n",
        "            if (liveNeighbors < 1 || liveNeighbors > 5) {\n",
        "              newGameState[i][j] = 0;\n",
        "            } else {\n",
        "              newGameState[i][j] = 1;\n",
        "            }\n",
        "          } else {\n",
        "            if (liveNeighbors === 3) {\n",
        "              newGameState[i][j] = 1;\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      gameState = newGameState;\n",
        "    }\n",
        "\n",
        "    // Main game loop\n",
        "    function gameLoop() {\n",
        "      updateGameState();\n",
        "      drawGame();\n",
        "      requestAnimationFrame(gameLoop);\n",
        "    }\n",
        "\n",
        "    // Initialize the game state and start the game loop\n",
        "    initializeGameState();\n",
        "    gameLoop();\n",
        "  </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGiKW1cT_BQS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzgdgxlkusGs"
      },
      "outputs": [],
      "source": [
        "prompt_fixer_job = tasker_prompt\n",
        "prompt_fixer_result = create_worker(llamaChat, prompt_fixer_prompt, prompt_fixer_job)\n",
        "print(prompt_fixer_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AI Society\n",
        "# Import necessary libraries\n",
        "from gradio_client import Client\n",
        "\n",
        "# Function to create AI society worker instances\n",
        "# (ysharma)Parameter 3 is the token length. 512 is short. 4096 is max?\n",
        "# (huggingface-projects)Has 7 parameters.\n",
        "###Curently commenting out different workers until I can figure out how to automatically know which apiset to use.\n",
        "\n",
        "###Using ysharma(llama2) - This one works, but uses up gpu fast.\n",
        "###def create_worker(api_name, initial_prompt, task_prompt):\n",
        "###    client = Client(api_name)\n",
        "###    result = client.predict(\n",
        "###        message=f\"{initial_prompt}\\n**Task:**\\n{task_prompt}\",\n",
        "###        request=0.95,\n",
        "###        param_3=512,\n",
        "###        api_name=\"/chat\"\n",
        "###    )\n",
        "###    return result\n",
        "\n",
        "#Using huggingface-projects(llama4) - Trying new space...\n",
        "def create_worker(api_name, initial_prompt, task_prompt):\n",
        "\n",
        "  client = Client(api_name)\n",
        "  result = client.predict(\n",
        "\t\tmessage=f\"**Job:**\\n{initial_prompt}\",\n",
        "\t\trequest=f\"\\n**Task:**\\n{task_prompt}\",\n",
        "\t\tparam_3=1024,\n",
        "\t\tparam_4=0.6,\n",
        "\t\tparam_5=0.9,\n",
        "\t\tparam_6=50,\n",
        "\t\tparam_7=1.2,\n",
        "\t\tapi_name=\"/chat\"\n",
        "\n",
        "  )\n",
        "  return result\n",
        "\n",
        "###################################################\n",
        "### INIT TASK PROMPTS TO PROJECT MANAGER\n",
        "###################################################\n",
        "#project_manager_job0=f\"read this script: {snippet}. Code in javascript. Implement a feature to tile the 80x80 canvas in an 8x8 grid where each grid position contains 1 instance of the game. In total there should be 64 canvasses arrayed in a grid, each canvas running the code snippet provided earlier.\"\n",
        "#project_manager_job1=f\"read this script: {snippet}. It was Coded in java, on the Processing 4 IDE. It is a game of life simulator using a unique ruleset to make tiling patterns. I need new features added to the script. 1a. A tileset saving function. 1b. A way to draw and save templates of drawings made with the MouseIsDown events. 2. A way to pause, reverse, and forward step events. 3. A way to export tilesets as gcode, to be interpreted by a slicing proram(Cura).\"\n",
        "#Implement a feature to tile the 80x80 canvas in an 8x8 grid where each grid position contains 1 instance of the game.\n",
        "#In total there should be 64 canvasses arrayed in a grid, each canvas running the code snippet provided earlier.\n",
        "project_manager_job=f\"read this script: {snippet}. It was Coded in java, on the Processing 4 IDE. It is a game of life simulator using a unique ruleset to make tiling patterns. I need new features added to the script. 1a. A tileset saving function. 1b. A way to draw and save templates of drawings made with the MouseIsDown events. 2. A way to pause, reverse, and forward step events. 3. A way to export tilesets as gcode, to be interpreted by a slicing proram(Cura).\"\n",
        "#Somewhere here we need to get the last pastebin results and loop it back to project manager job in the new snippet.\n",
        "#f=\"In this array {pastebin_list}, grab the most recent paste url, which will be the last item in the array. This is the previous tasks' results.\n",
        "#f=\"Read this last job's report: {final_result}.\"\"\n",
        "#project_manager_job=\n",
        "###################################################\n",
        "### COMMENTS\n",
        "###################################################\n",
        "#abacab\n",
        "# Define prompts for each role\n",
        "#output has this format\\n [1 sentence summary of assigned task] [results of task] [next steps for further improvement] [new task assignment]\n",
        "#interior inference  variables for taskers to follow, and code to interpret in [brackets]\n",
        "#Project Manager's internal variables\n",
        "###workers=[manager, tasker, coder, interpreter, reviewer, summarizer, delegater]\n",
        "#[inputProject] user prompt's contents\n",
        "#[project] commandified procedure of [inputProject]\n",
        "#[delegate]\n",
        "\n",
        "#Tasker's internal variables\n",
        "#[task] Job Assignment\n",
        "#[assummary] Assignment Summary\n",
        "#[results] Final results of assigned task\n",
        "#[nexttask] Next steps for further improvement\n",
        "#[delegate] workers to pass [nexttask] to\n",
        "#[memosend] Next worker's assignment instructions'\n",
        "\n",
        "#############################################################\n",
        "### PEPTALKS\n",
        "#############################################################\n",
        "#Interpreter's internal variables, their job role(coder, project manager, reviewer), and their role's duties.\n",
        "coder_prompt = \"You are the Coder AI. Your job is to write all code for the project. initially, javacoder recieves a program outline from the project manager. the program outline contains in comments @placeholders for the program's functions. Afterward, focus on the @placeholders and write the code described for each of the @placeholders as concisely and completly as possible. Finally, remove the @placeholder comment ONLY for the codeblock you've completed. Continue for the rest of the @placeholders yet to be coded, unitil you've written code for ALL of the @placeholder tasks. send the results to the reviewer.\"\n",
        "reviewer_prompt = \"You are the Code Reviewer AI. Your job is to scan and fix the code for errors. Afterward, print the entire contents of script in a code box labeled OUTPUT_SCRIPT. Finally,  if there are any @placeholders remaining, declare the program is not yet complete.  Only if there are no @placeholders remaining, state the program is fully complete.\"\n",
        "tasker_prompt = \"You are the Tasker AI. Your job is to assign tasks to different AI workers based on incoming information. Provide task instructions and send 'result' to the summarizer.\"\n",
        "project_manager_fired = \"You are the Project Manager AI. Your job is to oversee the progress of tasks assigned by the Tasker AI, manage resources, and coordinate with other AI workers. Send 'result' to the summarizer.\"\n",
        "project_manager_prompt = \"You are the java Admin AI. Your job is to create an outline script containing commented concise instruction prompts for the needed code. All output should be in code. outline the basic functions and name variables. You will then write @placeholder comments. These comments will describe what still needs to be written in the outlined script.\"\n",
        "input_output_prompt = \"You are the Input-Output AI. Your job is to process incoming information, interact with external systems, and provide outputs for further processing. Send 'result' to the summarizer.\"\n",
        "prompt_fixer_prompt = \"You are the Prompt Fixer AI. Your job is critical in ensuring efficient communication with our Large Language Models (LLMs). Your task is to analyze and refine prompts submitted for LLM inference, making them concise and effective while preserving the core intent. Afterwards, use the template to append your output.\"\n",
        "boolean_dude_prompt = \"you are a boolean AI. Evaluate: does the reviewer declare the program is complete? your only output shall be a 0 if the reviewer declares the program is not complete, or a 1 if the reviewer declares the program is complete. Thouroughly read the reviewers output outlined in **task**. only reply with 0 or 1\"\n",
        "prompts_array = [tasker_prompt, project_manager_prompt, input_output_prompt, prompt_fixer_prompt]\n",
        "\n",
        "#############################################################\n",
        "### LinkDen\n",
        "#############################################################\n",
        "#Spaces API clients\n",
        "llamaChat2=\"ysharma/Chat_with_Meta_llama3_8b\"\n",
        "llamaChat4=\"huggingface-projects/llama-2-7b-chat\"\n",
        "llamaChat3=\"huggingface-projects/llama-2-13b-chat\"\n",
        "llamaChat=\"eswardivi/llama3-8b-dhenu-0.1\"\n",
        "\n",
        "#############################################################\n",
        "### INIT VARIABLES\n",
        "#############################################################\n",
        "tasker_job = \"\"\n",
        "input_output_job = \"\"\n",
        "prompt_fixer_job = tasker_prompt\n",
        "\n",
        "#if not pastebin_list:\n",
        "#  pastebin_list = []\n",
        "\n",
        "\n",
        "############################################################\n",
        "### ATTACH BRAINS TO WORKERS\n",
        "############################################################\n",
        "# Create instances of different AI workers\n",
        "#tasker_result = create_worker(llamaChat, tasker_prompt, tasker_job)\n",
        "project_manager_result = create_worker(llamaChat4, project_manager_prompt, project_manager_job)\n",
        "coder_result = create_worker(llamaChat4, coder_prompt, project_manager_result)\n",
        "reviewer_result = create_worker(llamaChat4, reviewer_prompt, coder_result)\n",
        "boolean_result = create_worker(llamaChat4, boolean_dude_prompt, reviewer_result)\n",
        "#input_output_result = create_worker(llamaChat, input_output_prompt, task_prompt)\n",
        "#prompt_fixer_result = create_worker(llamaChat, prompt_fixer_prompt, prompt_fixer_job)\n",
        "\n",
        "coder_job=project_manager_result\n",
        "reviewer_job=coder_result\n",
        "boolean_job = \"\"\n",
        "\n",
        "#######################################################################\n",
        "### SUMMARIZER AI\n",
        "#######################################################################\n",
        "# Function to collate all called tasks' results, title the output's worker.\n",
        "# Prettyprintifying, please wait...\n",
        "def summarizer(project_manager_result, coder_result, reviewer_result):\n",
        "    # Summarize results and infer future tasks\n",
        "    summarized_result = f\"\"\"\n",
        "    Manager AI: {project_manager_result}\n",
        "    Coder AI: {coder_result}\n",
        "    Reviewer AI: {reviewer_result}\n",
        "    \"\"\"\n",
        "    return summarized_result\n",
        "\n",
        "# Call the summarizer function\n",
        "final_result = summarizer(project_manager_result, coder_result, reviewer_result)\n",
        "\n",
        "#summarizer_prompt = final_result+\"Summarize this workflow in a single paragraph.\"\n",
        "#summarizer_result = create_worker(llamaChat, summarizer_prompt, task_prompt)\n",
        "\n",
        "# Print or further process the final result\n",
        "print(final_result)\n",
        "\n",
        "#############################################################\n",
        "### BOOLEAN DUDE AI\n",
        "#############################################################\n",
        "# Function to check Boolean Dude's response for 0 or 1 contained in text, returns True or False\n",
        "# This complexication of inference is for further project workflow\n",
        "# FLOWCHARTS WHEN????\n",
        "\n",
        "print(f\"Based on the inputs received, boolean dude has said {boolean_result}.\")\n",
        "def boolean_dude_response(boolean_result):\n",
        "  #Check if Boolean Dude has said \"0\", return False\n",
        "  if \"0\" in boolean_result:\n",
        "    return False\n",
        "  # Check if the string contains \"1\" (only if \"0\" wasn't found earlier)\n",
        "  elif \"1\" in boolean_result:\n",
        "    return True\n",
        "is_project_done = bool(boolean_result)\n",
        "if is_project_done == False:\n",
        "  project_manager_job.append(f\"Here's a previous task that needs more work: {final_result}\")\n",
        "\n",
        "####################################################################"
      ],
      "metadata": {
        "id": "6ad76NAKlHG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0vBizwslmUj"
      },
      "outputs": [],
      "source": [
        "#prompt_fixer_result = create_worker(llamaChat, prompt_fixer_prompt, prompt_fixer_job)\n",
        "#print(prompt_fixer_result)\n",
        "prompts_array = [tasker_prompt, project_manager_prompt, input_output_prompt, prompt_fixer_prompt]\n",
        "# Iterate through prompts and call prompt fixer\n",
        "for prompt in prompts_array:\n",
        "  print(prompt)\n",
        "\n",
        "  prompt_fixer_output_template =f\"**Template** task: <task>\\nresult:<result>\\n**Explanation:**\\n\\n- The output starts with <task> to indicate the assigned task.\\n- It uses arrow brackets (`<>`) to mark the placeholders for `task` and `result`.\\n'task' prints out the provided task instructions from prompt. Clearly define the task you have been assigned to perform.\\n'result'prints out only the results from the analysis and completion of the task.\"\n",
        "  prompt_fixer_init_and_template = f\"{prompt_fixer_prompt}\\nUse this Output Template to organize output:\\n{prompt_fixer_output_template}\"\n",
        "  prompt_fixer_result = create_worker(llamaChat, prompt_fixer_init_and_template, prompt)\n",
        "  prompt_fixer_printout = f\"Original Prompt: {prompt}\\nFixed Prompt: {prompt_fixer_result}\\n\"\n",
        "  print(prompt_fixer_printout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4fDczZ8g1cb"
      },
      "outputs": [],
      "source": [
        "print(summarizer_result)\n",
        "\n",
        "\n",
        "summarizer_prompt = final_result+\"Summarize this workflow in a single paragraph.\"\n",
        "summarizer_result = create_worker(llamaChat, summarizer_prompt, task_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4dhONyRUked"
      },
      "source": [
        "# Import necessary libraries\n",
        "from gradio_client import Client\n",
        "\n",
        "# Function to create AI society worker instances\n",
        "def create_worker(api_name, initial_prompt):\n",
        "    client = Client(api_name)\n",
        "    result = client.predict(\n",
        "        message=initial_prompt,\n",
        "        request=0.95,\n",
        "        param_3=512,\n",
        "        api_name=\"/chat\"\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Define prompts for each role\n",
        "tasker_prompt = \"You are the Tasker AI. Your job is to assign tasks to different AI workers based on incoming information. Provide task instructions and send 'result' to the summarizer.\"\n",
        "project_manager_prompt = \"You are the Project Manager AI. Your job is to oversee the progress of tasks assigned by the Tasker AI, manage resources, and coordinate with other AI workers. Send 'result' to the summarizer.\"\n",
        "input_output_prompt = \"You are the Input-Output AI. Your job is to process incoming information, interact with external systems, and provide outputs for further processing. Send 'result' to the summarizer.\"\n",
        "\n",
        "# Create instances of different AI workers\n",
        "tasker_result = create_worker(\"ysharma/Chat_with_Meta_llama3_8b\", tasker_prompt)\n",
        "project_manager_result = create_worker(\"ysharma/Chat_with_Meta_llama3_8b\", project_manager_prompt)\n",
        "input_output_result = create_worker(\"ysharma/Chat_with_Meta_llama3_8b\", input_output_prompt)\n",
        "\n",
        "# Function to summarize and infer future tasks\n",
        "def summarizer(tasker_result, project_manager_result, input_output_result):\n",
        "    # Summarize results and infer future tasks\n",
        "    summarized_result = f\"Tasker AI: {tasker_result}\\nProject Manager AI: {project_manager_result}\\nInput-Output AI: {input_output_result}\\n\\nBased on the inputs received, the next steps could be inferred here.\"\n",
        "    return summarized_result\n",
        "\n",
        "# Call the summarizer function\n",
        "final_result = summarizer(tasker_result, project_manager_result, input_output_result)\n",
        "\n",
        "# Print or further process the final result\n",
        "print(final_result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNZ8SPJGQtbmXPUMvoios7L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}